{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import pickle\n",
    "from math import log\n",
    "\n",
    "# dataset source\n",
    "# https://files.pushshift.io/reddit/comments/\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditThread():\n",
    "    def __init__(self, link):\n",
    "        self.body = \"\"\n",
    "        self.netScore = 0\n",
    "        self.link = link\n",
    "        self.numberOfComments = 0\n",
    "\n",
    "    def update(self, body, score):\n",
    "        self.netScore += score\n",
    "        self.body = self.body + \" \" + body\n",
    "        self.numberOfComments += 1\n",
    "\n",
    "    def concat(self, reddit_thread):\n",
    "        self.netScore += reddit_thread.netScore\n",
    "        self.body = self.body + \" \" + reddit_thread.body\n",
    "        self.numberOfComments += reddit_thread.numberOfComments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicsSubreddit = dict()\n",
    "with open('politics.pkl', 'rb') as politics:\n",
    "    politicsSubreddit = pickle.load(politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builda dict of thread bodies by link_id\n",
    "\n",
    "def build_thread_bodies(threads):\n",
    "    thread_bodies = dict() # link_id: 'str w/ all comment text'\n",
    "    for link_id in threads.keys():\n",
    "        thread_bodies[link_id] = threads[link_id].body\n",
    "            \n",
    "        # PARSE RULES #\n",
    "        thread_bodies[link_id] = thread_bodies[link_id].strip()\n",
    "        thread_bodies[link_id] = thread_bodies[link_id].lower()\n",
    "        thread_bodies[link_id] = thread_bodies[link_id].replace('\\r\\n\\r\\n', ' ')\n",
    "        thread_bodies[link_id] = thread_bodies[link_id].replace('&gt;', '')\n",
    "        thread_bodies[link_id] = thread_bodies[link_id].replace('\\r', '')\n",
    "        thread_bodies[link_id] = thread_bodies[link_id].replace('\\n', '')\n",
    "    return thread_bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print('Topic {}:'.format(topic_idx))\n",
    "        print(' '.join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Peterli/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "law speech court rights constitution\n",
      "\n",
      "Topic 1:\n",
      "http com www reddit amp\n",
      "\n",
      "Topic 2:\n",
      "health care insurance people don\n",
      "\n",
      "Topic 3:\n",
      "war people military world just\n",
      "\n",
      "Topic 4:\n",
      "people just don like think\n",
      "\n",
      "Topic 5:\n",
      "fox news deleted paul conservative\n",
      "\n",
      "Topic 6:\n",
      "people just don like think\n",
      "\n",
      "Topic 7:\n",
      "obama party vote republicans democrats\n",
      "\n",
      "Topic 8:\n",
      "tax money taxes pay government\n",
      "\n",
      "Topic 9:\n",
      "people government corporations don think\n",
      "\n",
      "time elapsed: 82.4831018447876 seconds\n"
     ]
    }
   ],
   "source": [
    "NUM_TOPICS = 10\n",
    "NUM_WORDS = 5\n",
    "PASSES = 20\n",
    "\n",
    "thread_bodies = build_thread_bodies(politicsSubreddit)\n",
    "\n",
    "# train LDA model\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "train = list(thread_bodies.values())\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "tf_train = tf_vectorizer.fit_transform(train)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_topics=NUM_TOPICS, \n",
    "    max_iter=PASSES, \n",
    "    learning_method='online', \n",
    "    learning_offset=50., \n",
    "    random_state=0\n",
    ")\n",
    "lda.fit(tf_train)\n",
    "\n",
    "display_topics(lda, tf_feature_names, NUM_WORDS)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('time elapsed: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = list(thread_bodies.values())\n",
    "tf_test = tf_vectorizer.fit_transform(test_set)\n",
    "predict = lda.transform(tf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions available for 8970 elements\n",
      "\n",
      "array([[3.61112955e-04, 2.51541229e-01, 5.42277878e-02, ...,\n",
      "        2.38890274e-01, 1.46677149e-03, 3.61110265e-04],\n",
      "       [8.41956160e-05, 2.23517748e-01, 8.41951202e-05, ...,\n",
      "        1.47583522e-02, 8.41948160e-05, 8.19244213e-03],\n",
      "       [4.22662134e-02, 2.56507499e-04, 2.56462925e-04, ...,\n",
      "        2.56517561e-04, 2.56478763e-04, 2.56518968e-04],\n",
      "       ...,\n",
      "       [1.42893350e-02, 1.42920633e-02, 1.42882338e-02, ...,\n",
      "        1.42888607e-02, 1.42887527e-02, 2.92974422e-01],\n",
      "       [2.00022039e-02, 8.19973476e-01, 2.00023793e-02, ...,\n",
      "        2.00034263e-02, 2.00026254e-02, 2.00024164e-02],\n",
      "       [3.33369280e-02, 3.33357393e-02, 3.33374851e-02, ...,\n",
      "        3.33372939e-02, 3.33367722e-02, 3.33423836e-02]])\n"
     ]
    }
   ],
   "source": [
    "print('predictions available for {} elements\\n'.format(len(predict)))\n",
    "pp.pprint(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 1.5037338733673096 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create matrix that stores cosine distances between predictions\n",
    "# We use this because we don't care about the length of the vectors\n",
    "cos_dist_matrix = cosine_distances(predict, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05918912732485639\n"
     ]
    }
   ],
   "source": [
    "# Calculate the distance threshold we should use\n",
    "# Currently std is really high and makes our graph very crowded\n",
    "# What is a good way to do this\n",
    "DIST_THRES = np.std(cos_dist_matrix) / 5\n",
    "print(DIST_THRES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max distance seen: 0.9998274146600503\n",
      "0 mistakes out of 80460900 entries\n",
      "time elapsed: 202.29593014717102 seconds\n"
     ]
    }
   ],
   "source": [
    "# ASSUMPTION: dict.keys() and dict.values() directly correspond if not modified\n",
    "# rehydrate node_ids into dist_matrix -> distance lookup table\n",
    "\n",
    "max_dist = float('-inf')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "dist_lookup = { node_id: {} for node_id in thread_bodies.keys() }\n",
    "for idx1, first in enumerate(list(dist_lookup.keys())):\n",
    "    for idx2, second in enumerate(list(dist_lookup.keys())):\n",
    "        max_dist = cos_dist_matrix[idx1][idx2] if cos_dist_matrix[idx1][idx2] > max_dist else max_dist\n",
    "        dist_lookup[first][second] = cos_dist_matrix[idx1][idx2] # lazy eval\n",
    "\n",
    "# check\n",
    "\n",
    "mistakes = 0\n",
    "for i in dist_lookup.keys():\n",
    "    for j in dist_lookup.keys():\n",
    "        if i==j and dist_lookup[i][j] != 0:\n",
    "            print('something is off about dist_lookup[{}][{}] value {}'.format(i,j,dist_lookup[i][j]))\n",
    "            mistakes += 1\n",
    "        if dist_lookup[i][j] != dist_lookup[j][i]:\n",
    "            mistakes += 1\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('max distance seen: {}'.format(max_dist))\n",
    "print('{} mistakes out of {} entries'.format(mistakes, len(dist_lookup) * len(dist_lookup)))\n",
    "print('time elapsed: {} seconds'.format(end - start)) # 3mins to run on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k = 15\n",
    "\n",
    "# construct network\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "g = nx.Graph()\n",
    "for thread_id in thread_bodies.keys():\n",
    "    g.add_node(thread_id)\n",
    "\n",
    "for first in thread_bodies.keys():\n",
    "    dists_from_first = dist_lookup[first].items()\n",
    "    sorted_dists_from_first = sorted(dists_from_first, key=lambda x: x[1])\n",
    "    for (second, dist) in sorted_dists_from_first[:k]:\n",
    "        if not g.has_edge(first, second) and dist > 0.0:\n",
    "            g.add_edge(first, second)\n",
    "\n",
    "end = time.time()\n",
    "print('time elapsed: {} seconds'.format(end - start))\n",
    "        \n",
    "# for first in thread_bodies.keys():\n",
    "#     for second in thread_bodies.keys():\n",
    "#         if not g.has_edge(first, second) and dist_lookup[first][second] == 1:\n",
    "#             g.add_edge(first, second)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.is_connected(g))\n",
    "degs = nx.degree_histogram(g)\n",
    "# plt.hist(degs, normed=True, bins=len(degs))\n",
    "# nx.density(g)\n",
    "print(nx.info(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_show_visualization(g, file_name):\n",
    "    plt.figure(figsize=(18,18))\n",
    "    degrees = dict(nx.degree(g))\n",
    "    \n",
    "    # Draw networkx graph -- scale node size by log(degree+1)\n",
    "    nx.draw_spring(g, with_labels=False, \n",
    "                   linewidths=2.0,\n",
    "                   nodelist=degrees.keys(),\n",
    "                   node_size=[log(degree_val+1) * 100 for degree_val in degrees.values()], \\\n",
    "                   node_color='r')\n",
    "    \n",
    "    # Create black border around node shapes\n",
    "    ax = plt.gca()\n",
    "    ax.collections[0].set_edgecolor(\"#000000\")\n",
    "\n",
    "    # Save and show figure, then clear figure\n",
    "    plt.savefig(file_name)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize network\n",
    "start = time.time()\n",
    "save_and_show_visualization(g, \"politics.pdf\")\n",
    "end = time.time()\n",
    "print('time elapsed: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
