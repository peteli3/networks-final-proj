{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "# dataset source\n",
    "# https://files.pushshift.io/reddit/comments/\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access constants\n",
    "\n",
    "ID = 'id'\n",
    "BODY = 'body'\n",
    "SCORE = 'score'\n",
    "PARENT = 'parent_id'\n",
    "LINK = 'link_id'\n",
    "\n",
    "DATASET_FILE = 'reddit_2006jan.json'\n",
    "QUERY_SUBREDDIT = 'reddit.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, body, score, parent, link):\n",
    "        self.body = body\n",
    "        self.score = score\n",
    "        self.parent = parent\n",
    "        self.link = link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dict of Node Objects by post id\n",
    "\n",
    "def build_nodes_set(data):\n",
    "    nodes = dict() # post_id: Node Object\n",
    "    for post in data:\n",
    "        nodes[post[ID]] = Node(post[BODY], post[SCORE], post[PARENT], post[LINK])\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dict of post ids by link_id\n",
    "\n",
    "def build_threads_set(nodes):\n",
    "    threads = dict() # link_id: [post_ids]\n",
    "    for post_id in nodes.keys():\n",
    "        thread_id = nodes[post_id].link\n",
    "        if thread_id in threads.keys():\n",
    "            threads[thread_id].append(post_id)\n",
    "        else:\n",
    "            threads[thread_id] = [post_id]\n",
    "    return threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builda dict of thread bodies by link_id\n",
    "\n",
    "def build_thread_bodies(threads, nodes):\n",
    "    thread_bodies = dict() # link_id: 'str w/ all comment text'\n",
    "    for link_id in threads.keys():\n",
    "        thread_bodies[link_id] = \"\"\n",
    "        comments = threads[link_id]\n",
    "        for post_id in comments:\n",
    "            thread_bodies[link_id] += nodes[post_id].body\n",
    "\n",
    "        # PARSE RULES #\n",
    "        thread_bodies[link_id] = thread_bodies[link_id].replace('\\n', '')\n",
    "        thread_bodies[link_id] = thread_bodies[link_id].replace('\\r', '')\n",
    "    return thread_bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3664 results for query on subreddit: reddit.com\n",
      "\n",
      "0 entries without id\n",
      "0 entries without body\n",
      "0 entries without parent\n",
      "0 entries without link\n"
     ]
    }
   ],
   "source": [
    "# read dataset\n",
    "\n",
    "with open(DATASET_FILE) as file:\n",
    "    train_data = json.loads(file.read())\n",
    "\n",
    "# filter posts by subreddit\n",
    "\n",
    "train_data = [post for post in train_data if post['subreddit'] == QUERY_SUBREDDIT]\n",
    "print('{} results for query on subreddit: {}\\n'.format(len(train_data), QUERY_SUBREDDIT))\n",
    "\n",
    "# count up degenerate entries\n",
    "\n",
    "no_id = [post for post in train_data if not ID in post]\n",
    "print('{} entries without id'.format(len(no_id)))\n",
    "\n",
    "no_body = [post for post in train_data if not BODY in post]\n",
    "print('{} entries without body'.format(len(no_body)))\n",
    "\n",
    "no_parent = [post for post in train_data if not PARENT in post]\n",
    "print('{} entries without parent'.format(len(no_parent)))\n",
    "\n",
    "no_link = [post for post in train_data if not LINK in post]\n",
    "print('{} entries without link'.format(len(no_link)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim LDA library - ignore\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stop = set(get_stop_words('en'))\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "clean = []\n",
    "for idx, thread_body in enumerate(thread_bodies.values()):\n",
    "    raw_text = thread_body.lower()\n",
    "    tokens = tokenizer.tokenize(raw_text)\n",
    "    stopped = [i for i in tokens if i not in en_stop]\n",
    "    stemmed = [porter_stemmer.stem(i) for i in stopped]\n",
    "    clean.append(stemmed)\n",
    "\n",
    "# term_dict = corpora.Dictionary(clean)\n",
    "# corpus = [term_dict.doc2bow(text) for text in clean]\n",
    "\n",
    "# start = time.time()\n",
    "# lda = models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS, id2word=term_dict, passes=PASSES)\n",
    "# pp.pprint(lda.print_topics(num_topics=NUM_TOPICS, num_words=NUM_WORDS))\n",
    "# end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print('Topic {}:'.format(topic_idx))\n",
    "        print(' '.join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Peterli/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "deleted windows\n",
      "\n",
      "Topic 1:\n",
      "people like\n",
      "\n",
      "Topic 2:\n",
      "oil google\n",
      "\n",
      "Topic 3:\n",
      "just like\n",
      "\n",
      "Topic 4:\n",
      "http com\n",
      "\n",
      "time elapsed: 6.158347129821777 seconds\n"
     ]
    }
   ],
   "source": [
    "NUM_TOPICS = 5\n",
    "NUM_WORDS = 2\n",
    "PASSES = 20\n",
    "\n",
    "nodes = build_nodes_set(train_data)\n",
    "threads = build_threads_set(nodes)\n",
    "thread_bodies = build_thread_bodies(threads, nodes)\n",
    "\n",
    "# train LDA model\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "train = list(thread_bodies.values())\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "tf_train = tf_vectorizer.fit_transform(train)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=NUM_TOPICS, max_iter=PASSES, learning_method='online', learning_offset=50.,random_state=0)\n",
    "lda.fit(tf_train)\n",
    "\n",
    "display_topics(lda, tf_feature_names, NUM_WORDS)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('time elapsed: {} seconds'.format(end - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0.05001324, 0.79618753, 0.0500038 , 0.05082156, 0.05297387],\n",
      "       [0.00769538, 0.68456757, 0.044237  , 0.14198069, 0.12151936],\n",
      "       [0.33236697, 0.06884921, 0.06667538, 0.06713112, 0.46497733]])\n",
      "\n",
      "A look at Vietnam and Mexico exposes the myth of market liberalisation.\n",
      "\n",
      "The site states \"What can I use it for? Meeting notes, Reports, technical specs Sign-up sheets, proposals and much more...\", just like any other new breeed of sites that want us to store everything we have on the web. And they even guarantee multiple levels of security and encryption etc. But what prevents these web site operators fom accessing and/or stealing Meeting notes, Reports, technical specs Sign-up sheets, proposals and much more, for competitive or personal gains...? I am pretty sure that most of them are honest, but what's there to prevent me from setting up a good useful site and stealing all your data? Call me paranoid - I am.\n",
      "\n",
      "Jython related topics by Frank Wierzbicki\n",
      "\n",
      "\n",
      "array([[0.02500827, 0.55511703, 0.20182294, 0.19272072, 0.02533104],\n",
      "       [0.20128602, 0.03526352, 0.23868772, 0.48832032, 0.03644241],\n",
      "       [0.10000944, 0.59823876, 0.10078102, 0.10096808, 0.1000027 ]])\n",
      "\n",
      "It may be old, but it was discussed for an hour or more on Jerry Springer's Air America talk show recently.  Some folks took it seriously especially from the South.\n",
      "\n",
      "Yes and it looks like they had good reason not to trust you.\n",
      "\n",
      "Again.  Relax.  It's funny.  Sheesh.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test prediction \n",
    "\n",
    "with open('reddit_2005dec.json') as file:\n",
    "    test_data = json.loads(file.read())\n",
    "\n",
    "nodes = build_nodes_set(test_data)\n",
    "threads = build_threads_set(nodes)\n",
    "thread_bodies = build_thread_bodies(threads, nodes)\n",
    "\n",
    "comment_bodies = { node_id: node.body for (node_id, node) in nodes.items() }\n",
    "\n",
    "test_set = list(comment_bodies.values())\n",
    "tf_test = tf_vectorizer.fit_transform(test_set)\n",
    "predict = lda.transform(tf_test)\n",
    "\n",
    "pp.pprint(predict[:3])\n",
    "print('')\n",
    "print(test_set[0] + '\\n')\n",
    "print(test_set[1] + '\\n')\n",
    "print(test_set[2] + '\\n\\n')\n",
    "\n",
    "pp.pprint(predict[-3:])\n",
    "print('')\n",
    "print(test_set[-3] + '\\n')\n",
    "print(test_set[-2] + '\\n')\n",
    "print(test_set[-1] + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions available for 1075 elements\n",
      "\n",
      "array([[0.05001324, 0.79618753, 0.0500038 , 0.05082156, 0.05297387],\n",
      "       [0.00769538, 0.68456757, 0.044237  , 0.14198069, 0.12151936],\n",
      "       [0.33236697, 0.06884921, 0.06667538, 0.06713112, 0.46497733],\n",
      "       ...,\n",
      "       [0.02500827, 0.55511703, 0.20182294, 0.19272072, 0.02533104],\n",
      "       [0.20128602, 0.03526352, 0.23868772, 0.48832032, 0.03644241],\n",
      "       [0.10000944, 0.59823876, 0.10078102, 0.10096808, 0.1000027 ]])\n"
     ]
    }
   ],
   "source": [
    "print('predictions available for {} elements\\n'.format(len(predict)))\n",
    "pp.pprint(predict)\n",
    "\n",
    "# not sure if this is chill\n",
    "m, n = np.meshgrid(predict, predict)\n",
    "dist_matrix = abs(m - n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build adjacency list (lookup table)\n",
    "# ASSUMPTION: dict.keys() and dict.values() directly correspond if not modified\n",
    "\n",
    "DIST_THRES = 0.420\n",
    "\n",
    "# rehydrate node_ids into dist_matrix -> distance lookup table\n",
    "dist_lookup = { node_id: {} for node_id in comment_bodies.keys() }\n",
    "for idx1, first in enumerate(list(dist_lookup.keys())):\n",
    "    for idx2, second in enumerate(list(dist_lookup.keys())):\n",
    "        dist_lookup[first][second] = 1 if dist_matrix[idx1][idx2] <= DIST_THRES else 0\n",
    "\n",
    "# pp.pprint(dist_lookup)\n",
    "\n",
    "\n",
    "\n",
    "# print(list(comment_bodies.keys())[:2])\n",
    "# print(list(comment_bodies.values())[:2])\n",
    "\n",
    "# print(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
